{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ffceae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: snscrape 0.7.0.20230622\n",
      "Uninstalling snscrape-0.7.0.20230622:\n",
      "  Successfully uninstalled snscrape-0.7.0.20230622\n",
      "Collecting snscrape==0.4.3.20220106\n",
      "  Downloading snscrape-0.4.3.20220106-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from snscrape==0.4.3.20220106) (2.32.3)\n",
      "Requirement already satisfied: lxml in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from snscrape==0.4.3.20220106) (5.2.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from snscrape==0.4.3.20220106) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from snscrape==0.4.3.20220106) (3.13.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from beautifulsoup4->snscrape==0.4.3.20220106) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape==0.4.3.20220106) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape==0.4.3.20220106) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape==0.4.3.20220106) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape==0.4.3.20220106) (2025.1.31)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape==0.4.3.20220106) (1.7.1)\n",
      "Downloading snscrape-0.4.3.20220106-py3-none-any.whl (59 kB)\n",
      "Installing collected packages: snscrape\n",
      "Successfully installed snscrape-0.4.3.20220106\n",
      "Requirement already satisfied: pandas in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: fake-useragent in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "Scraping: #EndSARS education since:2020-10-08 until:2020-10-31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e48ea1c4b944836a8d0d5391a8ac61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&send_error_codes=true&simple_quoted_tweets=true&q=%23EndSARS+education+since%3A2020-10-08+until%3A2020-10-31&tweet_search_mode=live&count=100&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&ext=mediaStats%2ChighlightedLabel: non-200 status code\n",
      "4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&send_error_codes=true&simple_quoted_tweets=true&q=%23EndSARS+education+since%3A2020-10-08+until%3A2020-10-31&tweet_search_mode=live&count=100&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&ext=mediaStats%2ChighlightedLabel failed, giving up.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&send_error_codes=true&simple_quoted_tweets=true&q=%23EndSARS+education+since%3A2020-10-08+until%3A2020-10-31&tweet_search_mode=live&count=100&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&ext=mediaStats%2ChighlightedLabel failed, giving up.\n",
      "\n",
      "Scraping: #EndSARS school since:2020-10-08 until:2020-12-31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ed70d31a03468d80b8fc4000172c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&send_error_codes=true&simple_quoted_tweets=true&q=%23EndSARS+school+since%3A2020-10-08+until%3A2020-12-31&tweet_search_mode=live&count=100&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&ext=mediaStats%2ChighlightedLabel: non-200 status code\n",
      "4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&send_error_codes=true&simple_quoted_tweets=true&q=%23EndSARS+school+since%3A2020-10-08+until%3A2020-12-31&tweet_search_mode=live&count=100&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&ext=mediaStats%2ChighlightedLabel failed, giving up.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&send_error_codes=true&simple_quoted_tweets=true&q=%23EndSARS+school+since%3A2020-10-08+until%3A2020-12-31&tweet_search_mode=live&count=100&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&ext=mediaStats%2ChighlightedLabel failed, giving up.\n",
      "\n",
      "Scraping: #EndSARS university since:2021-10-01 until:2024-10-31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78d9241a4404919a7827c6ecb1f5497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&send_error_codes=true&simple_quoted_tweets=true&q=%23EndSARS+university+since%3A2021-10-01+until%3A2024-10-31&tweet_search_mode=live&count=100&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&ext=mediaStats%2ChighlightedLabel: non-200 status code\n",
      "4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&send_error_codes=true&simple_quoted_tweets=true&q=%23EndSARS+university+since%3A2021-10-01+until%3A2024-10-31&tweet_search_mode=live&count=100&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&ext=mediaStats%2ChighlightedLabel failed, giving up.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&send_error_codes=true&simple_quoted_tweets=true&q=%23EndSARS+university+since%3A2021-10-01+until%3A2024-10-31&tweet_search_mode=live&count=100&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&ext=mediaStats%2ChighlightedLabel failed, giving up.\n",
      "\n",
      "‚úÖ Twitter data saved (0 tweets)\n",
      "\n",
      "Scraping: PremiumTimes\n",
      "\n",
      "Scraping: Vanguard\n",
      "\n",
      "Scraping: Guardian\n",
      "\n",
      "‚úÖ News data saved (25 articles)\n",
      "Verification failed: No columns to parse from file\n",
      "\n",
      "‚ö†Ô∏è Some data collection failed - check logs\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # #EndSARS Data Collection (Fixed Version)\n",
    "# \n",
    "# **Resolved snscrape compatibility issue**\n",
    "\n",
    "# %% [code] -- FIXED INSTALLATION --\n",
    "# Uninstall problematic packages and install compatible versions\n",
    "!pip uninstall -y snscrape\n",
    "!pip install snscrape==0.4.3.20220106\n",
    "!pip install pandas requests beautifulsoup4 fake-useragent tqdm\n",
    "\n",
    "# %% [code] -- FIXED TWITTER IMPORT --\n",
    "# ======================\n",
    "# TWITTER DATA SCRAPING (FIXED)\n",
    "# ======================\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import snscrape.base  # Import base module first\n",
    "\n",
    "# Use COMPATIBLE import method\n",
    "from snscrape.modules import twitter as sntwitter\n",
    "\n",
    "# Configure search\n",
    "search_terms = [\n",
    "    \"#EndSARS education since:2020-10-08 until:2020-10-31\",\n",
    "    \"#EndSARS school since:2020-10-08 until:2020-12-31\",\n",
    "    \"#EndSARS university since:2021-10-01 until:2024-10-31\"\n",
    "]\n",
    "\n",
    "def scrape_twitter(terms, max_tweets=1000):\n",
    "    tweets = []\n",
    "    for term in terms:\n",
    "        print(f\"\\nScraping: {term}\")\n",
    "        try:\n",
    "            scraper = sntwitter.TwitterSearchScraper(term)\n",
    "            for i, tweet in tqdm(enumerate(scraper.get_items()), total=max_tweets):\n",
    "                if i >= max_tweets:\n",
    "                    break\n",
    "                tweets.append({\n",
    "                    \"date\": tweet.date,\n",
    "                    \"id\": tweet.id,\n",
    "                    \"content\": tweet.content,\n",
    "                    \"username\": tweet.user.username,\n",
    "                    \"likes\": tweet.likeCount,\n",
    "                    \"retweets\": tweet.retweetCount,\n",
    "                    \"hashtags\": tweet.hashtags\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "    return pd.DataFrame(tweets)\n",
    "\n",
    "# Run Twitter scraper (reduced count for testing)\n",
    "twitter_df = scrape_twitter(search_terms, max_tweets=500)\n",
    "\n",
    "# Save Twitter data\n",
    "twitter_df.to_csv(\"endsars_twitter_data.csv\", index=False)\n",
    "print(f\"\\n‚úÖ Twitter data saved ({len(twitter_df)} tweets)\")\n",
    "\n",
    "# %% [code]\n",
    "# ===================\n",
    "# NEWS DATA SCRAPING\n",
    "# ===================\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "# Initialize random user agents\n",
    "ua = UserAgent()\n",
    "\n",
    "sources = {\n",
    "    \"PremiumTimes\": \"https://www.premiumtimesng.com/tag/endsars/\",\n",
    "    \"Vanguard\": \"https://www.vanguardngr.com/tag/endsars/\",\n",
    "    \"Guardian\": \"https://guardian.ng/tag/endsars/\"\n",
    "}\n",
    "\n",
    "# Site-specific configuration\n",
    "site_config = {\n",
    "    \"PremiumTimes\": {\n",
    "        \"article_selector\": \"article\",\n",
    "        \"title_selector\": \"h2.headline\",\n",
    "        \"excerpt_selector\": \".excerpt\",\n",
    "        \"link_selector\": \"h2 a\"\n",
    "    },\n",
    "    \"Vanguard\": {\n",
    "        \"article_selector\": \".mvp-blog-story-wrap\",\n",
    "        \"title_selector\": \"h2\",\n",
    "        \"excerpt_selector\": \".mvp-blog-excerpt\",\n",
    "        \"link_selector\": \"h2 a\"\n",
    "    },\n",
    "    \"Guardian\": {\n",
    "        \"article_selector\": \".js-article-list-item\",\n",
    "        \"title_selector\": \"h3\",\n",
    "        \"excerpt_selector\": \".js-article-list-item__excerpt\",\n",
    "        \"link_selector\": \"h3 a\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def scrape_news_sites():\n",
    "    articles = []\n",
    "    for name, url in sources.items():\n",
    "        try:\n",
    "            print(f\"\\nScraping: {name}\")\n",
    "            headers = {'User-Agent': ua.random}\n",
    "            response = requests.get(url, headers=headers, timeout=15)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            config = site_config[name]\n",
    "            \n",
    "            for article in soup.select(config[\"article_selector\"]):\n",
    "                # Extract title\n",
    "                title_elem = article.select_one(config[\"title_selector\"])\n",
    "                title = title_elem.text.strip() if title_elem else ''\n",
    "                \n",
    "                # Extract excerpt\n",
    "                excerpt_elem = article.select_one(config[\"excerpt_selector\"])\n",
    "                excerpt = excerpt_elem.text.strip() if excerpt_elem else ''\n",
    "                \n",
    "                # Extract article URL\n",
    "                link_elem = article.select_one(config[\"link_selector\"])\n",
    "                if link_elem and link_elem.has_attr('href'):\n",
    "                    article_url = link_elem['href']\n",
    "                    # Fix relative URLs\n",
    "                    if not article_url.startswith('http'):\n",
    "                        article_url = url + article_url.lstrip('/')\n",
    "                else:\n",
    "                    article_url = url\n",
    "                \n",
    "                articles.append({\n",
    "                    \"source\": name,\n",
    "                    \"title\": title,\n",
    "                    \"excerpt\": excerpt,\n",
    "                    \"url\": article_url,\n",
    "                    \"source_page\": url\n",
    "                })\n",
    "            \n",
    "            # Respectful delay between sites\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {name}: {str(e)}\")\n",
    "    \n",
    "    return pd.DataFrame(articles)\n",
    "\n",
    "# Run news scraper\n",
    "news_df = scrape_news_sites()\n",
    "\n",
    "# Save news data\n",
    "if not news_df.empty:\n",
    "    news_df.to_csv(\"endsars_news_data.csv\", index=False)\n",
    "    print(f\"\\n‚úÖ News data saved ({len(news_df)} articles)\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No articles scraped - check selectors\")\n",
    "\n",
    "# %% [code]\n",
    "# =====================\n",
    "# DATA VERIFICATION\n",
    "# =====================\n",
    "def verify_data():\n",
    "    try:\n",
    "        # Load Twitter data\n",
    "        twitter_df = pd.read_csv(\"endsars_twitter_data.csv\")\n",
    "        print(f\"Twitter data: {len(twitter_df)} tweets\")\n",
    "        print(twitter_df.head(2))\n",
    "        \n",
    "        # Load News data\n",
    "        news_df = pd.read_csv(\"endsars_news_data.csv\")\n",
    "        print(f\"\\nNews data: {len(news_df)} articles\")\n",
    "        print(news_df.head(2))\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Verification failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "if verify_data():\n",
    "    print(\"\\nüéâ All data collected successfully!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some data collection failed - check logs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
